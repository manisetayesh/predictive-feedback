{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from models.mlp import *\n",
    "from models.hebbian import *\n",
    "from models.hybrid import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples retained:\n",
      "  279173 (training)\n",
      "  69793 (validation)\n",
      "  348966 (test)\n"
     ]
    }
   ],
   "source": [
    "def download_dataset(train_prop=0.8, keep_prop=0.5, ds_name=None):\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if ds_name is None:\n",
    "        download = os.path.exists(\"../data/MNIST/\")\n",
    "        full_train_set = torchvision.datasets.MNIST(\n",
    "            root=\"../data/\", train=True, download=download, transform=transform\n",
    "        )\n",
    "        full_test_set = torchvision.datasets.MNIST(\n",
    "            root=\"../data/\", train=False, transform=transform\n",
    "        )\n",
    "    elif ds_name == 'EMNIST':\n",
    "        # Might change split to \"balanced\" for balanced characters if performance is low\n",
    "        download = os.path.exists(\"../data/EMNIST/\")\n",
    "\n",
    "        full_train_set = torchvision.datasets.EMNIST(\n",
    "            root=\"../data/\", split=\"byclass\", download=download, transform=transform\n",
    "        )\n",
    "        full_test_set = torchvision.datasets.EMNIST(\n",
    "            root=\"../data/\", split=\"byclass\", download=download, transform=transform\n",
    "        )\n",
    "    else:\n",
    "        download = os.path.exists(\"../data/omniglot-py/\")\n",
    "\n",
    "        full_train_set = torchvision.datasets.Omniglot(\n",
    "            root=\"../data/\", download=download, transform=transform\n",
    "        )\n",
    "        full_test_set = torchvision.datasets.Omniglot(\n",
    "            root=\"../data/\", download=download, transform=transform\n",
    "        )\n",
    "    train_set, valid_set, _ = torch.utils.data.random_split(\n",
    "        full_train_set, [train_prop * keep_prop, (1 - train_prop) * keep_prop, 1-keep_prop]\n",
    "    )\n",
    "    test_set, _ = torch.utils.data.random_split(\n",
    "        full_test_set, [keep_prop, 1 - keep_prop]\n",
    "    )\n",
    "\n",
    "    print(\"Number of examples retained:\")\n",
    "    print(f\"  {len(train_set)} (training)\")\n",
    "    print(f\"  {len(valid_set)} (validation)\")\n",
    "    print(f\"  {len(test_set)} (test)\")\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "train_set, valid_set, test_set = download_dataset(ds_name='EMNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "NUM_HIDDEN = 100\n",
    "NUM_INPUTS = np.prod(train_set.dataset.data[0].shape)\n",
    "NUM_OUTPUTS = len(train_set.dataset.classes)  # number of classes\n",
    "ACTIVATION = \"sigmoid\"  # output constrained between 0 and 1\n",
    "BIAS = False\n",
    "MLP1 = MultiLayerPerceptron(\n",
    "    num_inputs=NUM_INPUTS,\n",
    "    num_outputs=NUM_OUTPUTS,\n",
    "    num_hidden=NUM_HIDDEN,\n",
    "    activation_type=ACTIVATION,\n",
    "    bias=BIAS,\n",
    ")\n",
    "ACTIVATION = \"relu\"\n",
    "MLP2 = MultiLayerPerceptron(\n",
    "    num_inputs=NUM_INPUTS,\n",
    "    num_outputs=NUM_OUTPUTS,\n",
    "    num_hidden=NUM_HIDDEN,\n",
    "    activation_type=ACTIVATION,\n",
    "    bias=BIAS,\n",
    ")\n",
    "\n",
    "hybrid_MLP = HybridMLP(MLP1, MLP2, 'weighted', 0.9)\n",
    "\n",
    "# Dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_set, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### FUNCTIONS ########\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, optimizer, num_epochs=5):\n",
    "    results_dict = {\n",
    "        \"avg_train_losses\": list(),\n",
    "        \"avg_valid_losses\": list(),\n",
    "        \"avg_train_accuracies\": list(),\n",
    "        \"avg_valid_accuracies\": list(),\n",
    "    }\n",
    "\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        no_train = True if e == 0 else False  # to get a baseline\n",
    "        latest_epoch_results_dict = train_epoch(\n",
    "            model, train_loader, valid_loader, optimizer=optimizer, no_train=no_train\n",
    "        )\n",
    "\n",
    "        for key, result in latest_epoch_results_dict.items():\n",
    "            if key in results_dict.keys() and isinstance(results_dict[key], list):\n",
    "                results_dict[key].append(latest_epoch_results_dict[key])\n",
    "            else:\n",
    "                results_dict[key] = result  # copy latest\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, valid_loader, optimizer, no_train=False):\n",
    "\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "\n",
    "    epoch_results_dict = dict()\n",
    "    for dataset in [\"train\", \"valid\"]:\n",
    "        for sub_str in [\"correct_by_class\", \"seen_by_class\"]:\n",
    "            epoch_results_dict[f\"{dataset}_{sub_str}\"] = {\n",
    "                i: 0 for i in range(model.num_outputs)\n",
    "            }\n",
    "\n",
    "    model.train()\n",
    "    train_losses, train_acc = list(), list()\n",
    "    for X, y in train_loader:\n",
    "        y_pred = model(X, y=y)\n",
    "        loss = criterion(torch.log(y_pred), y)\n",
    "        acc = (torch.argmax(y_pred.detach(), axis=1) == y).sum() / len(y)\n",
    "        train_losses.append(loss.item() * len(y))\n",
    "        train_acc.append(acc.item() * len(y))\n",
    "        update_results_by_class_in_place(\n",
    "            y,\n",
    "            y_pred.detach(),\n",
    "            epoch_results_dict,\n",
    "            dataset=\"train\",\n",
    "            num_classes=model.num_outputs,\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        if not no_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    num_items = len(train_loader.dataset)\n",
    "    epoch_results_dict[\"avg_train_losses\"] = np.sum(train_losses) / num_items\n",
    "    epoch_results_dict[\"avg_train_accuracies\"] = np.sum(train_acc) / num_items * 100\n",
    "\n",
    "    model.eval()\n",
    "    valid_losses, valid_acc = list(), list()\n",
    "    with torch.no_grad():\n",
    "        for X, y in valid_loader:\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(torch.log(y_pred), y)\n",
    "            acc = (torch.argmax(y_pred, axis=1) == y).sum() / len(y)\n",
    "            valid_losses.append(loss.item() * len(y))\n",
    "            valid_acc.append(acc.item() * len(y))\n",
    "            update_results_by_class_in_place(\n",
    "                y, y_pred.detach(), epoch_results_dict, dataset=\"valid\"\n",
    "            )\n",
    "\n",
    "    num_items = len(valid_loader.dataset)\n",
    "    epoch_results_dict[\"avg_valid_losses\"] = np.sum(valid_losses) / num_items\n",
    "    epoch_results_dict[\"avg_valid_accuracies\"] = np.sum(valid_acc) / num_items * 100\n",
    "\n",
    "    return epoch_results_dict\n",
    "\n",
    "\n",
    "def update_results_by_class_in_place(\n",
    "    y, y_pred, result_dict, dataset=\"train\", num_classes=10\n",
    "):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    for i in result_dict[f\"{dataset}_seen_by_class\"].keys():\n",
    "        idxs = np.where(y == int(i))[0]\n",
    "        result_dict[f\"{dataset}_seen_by_class\"][int(i)] += len(idxs)\n",
    "\n",
    "        num_correct = int(sum(y[idxs] == y_pred[idxs]))\n",
    "        result_dict[f\"{dataset}_correct_by_class\"][int(i)] += num_correct\n",
    "\n",
    "def evaluate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            y_pred = model(X)\n",
    "            predicted = torch.argmax(y_pred, axis=1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "LR = 0.01\n",
    "backprop_optimizer = BasicOptimizer(hybrid_MLP.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "MLP_results_dict = train_model(\n",
    "    hybrid_MLP,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer=backprop_optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    ")\n",
    "test_accuracy = evaluate_accuracy(hybrid_MLP, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
